# ğŸ§  mini-gpt-from-scratch

A minimal GPT-style language model â€” built 100% from scratch in PyTorch â€” trained to generate Shakespeare-like text.  
Includes full custom implementations of **Self-Attention**, **Multi-Head Attention**, **Masked Self-Attention**, and the complete GPT architecture.

---

## ğŸ“‚ Whatâ€™s Inside

| File | Description |
|------|--------------|
| `coding-self-attention-in-pytroch.ipynb` | Step-by-step notebook to implement **Self-Attention** from scratch |
| `coding-multi-head-attention-in-pytorch.ipynb` | Notebook for **Multi-Head Attention** implementation |
| `coding-masked-self-attention-in-pytroch.ipynb` | Notebook showing how **Masked Self-Attention** works for autoregressive text generation |
| `mini-gpt-from-scratch-in-pytorch.ipynb` | Complete **GPT model**: embeddings, positional encoding, attention blocks, feedforward layers, and text generation loop |
| `mini-gpt_from_scratch_in_pytorch.pth` | Trained GPT model weights |

---

## ğŸš€ Project Overview

This repo breaks down how a GPT works â€” piece by piece:
- **Self-Attention:** Learn how each token attends to others.
- **Multi-Head Attention:** Extend attention to multiple heads for richer representations.
- **Masked Attention:** Ensure the model predicts the next token without cheating.
- **Full GPT:** Assemble all blocks into a working generative language model.

I trained this GPT on a small Shakespeare dataset. Despite limited data, the model can generate short poetic lines that show the transformerâ€™s power.

---

## âœ¨ Sample Output

Example generated text:



---

## âš™ï¸ Requirements

- Python 3.x
- PyTorch
- Jupyter Notebook

---

## ğŸƒâ€â™‚ï¸ Run It Yourself

1ï¸âƒ£ Clone the repo:
```bash
git clone https://github.com/yourusername/mini-gpt-from-scratch.git
cd mini-gpt-from-scratch
pip install torch notebook
jupyter notebook
```

---

## ğŸ“š How I Built It

- Started by understanding the original transformer paper.
- Broke it down into small parts: self-attention â†’ multi-head â†’ masking.
- Rebuilt the whole GPT block by block.
- Trained on Shakespeareâ€™s text.
- Saved the final weights as `.pth`.

---

## ğŸ“« Connect

If you find this helpful or want to learn more â€” feel free to reach out or fork the project.  
**Letâ€™s build real AI, one block at a time.**

---

