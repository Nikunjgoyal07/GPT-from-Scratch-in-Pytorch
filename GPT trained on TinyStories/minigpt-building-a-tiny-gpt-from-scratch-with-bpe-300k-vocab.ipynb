{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12575948,"sourceType":"datasetVersion","datasetId":7942293},{"sourceId":487194,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":388566,"modelId":407501}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:25:52.977755Z","iopub.execute_input":"2025-07-26T08:25:52.978049Z","iopub.status.idle":"2025-07-26T08:25:53.293374Z","shell.execute_reply.started":"2025-07-26T08:25:52.978026Z","shell.execute_reply":"2025-07-26T08:25:53.292720Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/tokeniser/bpe_tokenizer_10k_vocab.json\n/kaggle/input/bpe-model-with-tinystories/pytorch/default/1/gpt_from_scratch_in_pytorch_on_stories_generation_v2.pth\n/kaggle/input/bpe-model-with-tinystories/pytorch/default/1/my_tokenizer.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#vocab_size = 4000\n#max_len_list = 1119","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:34:15.944676Z","iopub.execute_input":"2025-07-22T09:34:15.944929Z","iopub.status.idle":"2025-07-22T09:34:15.948708Z","shell.execute_reply.started":"2025-07-22T09:34:15.944913Z","shell.execute_reply":"2025-07-22T09:34:15.947901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:10:37.711366Z","iopub.execute_input":"2025-07-22T09:10:37.711932Z","iopub.status.idle":"2025-07-22T09:10:42.294357Z","shell.execute_reply.started":"2025-07-22T09:10:37.711913Z","shell.execute_reply":"2025-07-22T09:10:42.293629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"roneneldan/TinyStories\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:26:41.932632Z","iopub.execute_input":"2025-07-26T08:26:41.933041Z","iopub.status.idle":"2025-07-26T08:26:57.383539Z","shell.execute_reply.started":"2025-07-26T08:26:41.933018Z","shell.execute_reply":"2025-07-26T08:26:57.382624Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8752154bb4c8475891b552d81c2512cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00004-2d5a1467fff1081b.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa96c56fab274382a87d4fd2f19442e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00001-of-00004-5852b56a2bd28fd9.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f511936215ab4247be1dd809e609a224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00002-of-00004-a26307300439e943.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d9ee11bf8844fa9989b6df22f36b91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00003-of-00004-d243063613e5a057.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7724d13ef34ebd99b012fd02b458d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-869c898b519ad725.parquet:   0%|          | 0.00/9.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d64d31b5d440ba91a78c15055d335d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f601ab3fcb554607be9096c0049af161"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be17a475f974c8fb5350a5f563033fb"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"len(dataset[\"train\"]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:10:19.448143Z","iopub.execute_input":"2025-07-25T16:10:19.448669Z","iopub.status.idle":"2025-07-25T16:10:24.825351Z","shell.execute_reply.started":"2025-07-25T16:10:19.448641Z","shell.execute_reply":"2025-07-25T16:10:24.824659Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"2119719"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T07:59:40.823052Z","iopub.execute_input":"2025-07-26T07:59:40.823486Z","iopub.status.idle":"2025-07-26T07:59:40.828549Z","shell.execute_reply.started":"2025-07-26T07:59:40.823456Z","shell.execute_reply":"2025-07-26T07:59:40.827836Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2119719\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 21990\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset = dataset[\"train\"]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:26:57.384971Z","iopub.execute_input":"2025-07-26T08:26:57.385567Z","iopub.status.idle":"2025-07-26T08:27:03.126663Z","shell.execute_reply.started":"2025-07-26T08:26:57.385533Z","shell.execute_reply":"2025-07-26T08:27:03.125894Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset = dataset[:300000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:27:03.127557Z","iopub.execute_input":"2025-07-26T08:27:03.127775Z","iopub.status.idle":"2025-07-26T08:27:08.160456Z","shell.execute_reply.started":"2025-07-26T08:27:03.127757Z","shell.execute_reply":"2025-07-26T08:27:08.159653Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"len(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T16:10:40.756761Z","iopub.execute_input":"2025-07-25T16:10:40.757037Z","iopub.status.idle":"2025-07-25T16:10:40.771756Z","shell.execute_reply.started":"2025-07-25T16:10:40.757014Z","shell.execute_reply":"2025-07-25T16:10:40.770996Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"300000"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset2 = []\nfor i in dataset:\n    if len(i) > 100:\n        dataset2.append(i)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:50:05.879015Z","iopub.execute_input":"2025-07-25T14:50:05.879480Z","iopub.status.idle":"2025-07-25T14:50:09.315710Z","shell.execute_reply.started":"2025-07-25T14:50:05.879459Z","shell.execute_reply":"2025-07-25T14:50:09.314785Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"len(dataset2)\ndataset2[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:50:09.317155Z","iopub.execute_input":"2025-07-25T14:50:09.317512Z","iopub.status.idle":"2025-07-25T14:50:09.323024Z","shell.execute_reply.started":"2025-07-25T14:50:09.317486Z","shell.execute_reply":"2025-07-25T14:50:09.322196Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.',\n 'Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\\n\\nOne day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\\n\\nBeep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.']"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!pip install tokenizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:50:12.048700Z","iopub.execute_input":"2025-07-25T14:50:12.048979Z","iopub.status.idle":"2025-07-25T14:50:16.006478Z","shell.execute_reply.started":"2025-07-25T14:50:12.048956Z","shell.execute_reply":"2025-07-25T14:50:16.005507Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.6.15)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n\n# 1. Make a BPE tokenizer\ntokenizer = Tokenizer(models.BPE())\n\n# 2. Pre-tokenizer: ByteLevel is common for GPT-like BPE\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\n# 3. Decoder: ByteLevel to decode back\ntokenizer.decoder = decoders.ByteLevel()\n\n# 4. Trainer: define vocab size, min frequency, special tokens\ntrainer = trainers.BpeTrainer(\n    vocab_size=4000,  # adjust as needed\n    min_frequency=2,\n    special_tokens=[\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n)\n\n# 5. Train on your list of strings\ntokenizer.train_from_iterator(dataset2, trainer)\n\n# 6. Save the tokenizer if you want\ntokenizer.save(\"bpe_tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:26:25.144130Z","iopub.execute_input":"2025-07-22T09:26:25.144688Z","iopub.status.idle":"2025-07-22T09:26:28.483826Z","shell.execute_reply.started":"2025-07-22T09:26:25.144665Z","shell.execute_reply":"2025-07-22T09:26:28.483175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n\nfrom transformers import PreTrainedTokenizerFast\n\ntokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/kaggle/input/tokeniser/bpe_tokenizer_10k_vocab.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:27:08.162868Z","iopub.execute_input":"2025-07-26T08:27:08.163183Z","iopub.status.idle":"2025-07-26T08:27:17.790660Z","shell.execute_reply.started":"2025-07-26T08:27:08.163157Z","shell.execute_reply":"2025-07-26T08:27:17.789796Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:27:17.791520Z","iopub.execute_input":"2025-07-26T08:27:17.792078Z","iopub.status.idle":"2025-07-26T08:27:17.797876Z","shell.execute_reply.started":"2025-07-26T08:27:17.792050Z","shell.execute_reply":"2025-07-26T08:27:17.796930Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"10000"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"encoded = tokenizer.encode(dataset2[0])\nencoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T11:41:36.755682Z","iopub.execute_input":"2025-07-24T11:41:36.756268Z","iopub.status.idle":"2025-07-24T11:41:36.762057Z","shell.execute_reply.started":"2025-07-24T11:41:36.756246Z","shell.execute_reply":"2025-07-24T11:41:36.761298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_dataset = tokenizer(dataset2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:57:38.502021Z","iopub.execute_input":"2025-07-25T14:57:38.502689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded_dataset = []\nfor sentence in dataset:\n    encoded = tokenizer.encode(sentence)\n    encoded_dataset.extend(encoded)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:28:14.557546Z","iopub.execute_input":"2025-07-26T08:28:14.558071Z","iopub.status.idle":"2025-07-26T08:30:53.715274Z","shell.execute_reply.started":"2025-07-26T08:28:14.558044Z","shell.execute_reply":"2025-07-26T08:30:53.714646Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(encoded_dataset[0:2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:33:23.701122Z","iopub.execute_input":"2025-07-26T08:33:23.702186Z","iopub.status.idle":"2025-07-26T08:33:23.706518Z","shell.execute_reply.started":"2025-07-26T08:33:23.702158Z","shell.execute_reply":"2025-07-26T08:33:23.705513Z"}},"outputs":[{"name":"stdout","text":"[505, 254]\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"merged_dataset = []\nfor sentence in encoded_dataset:\n    merged_dataset.extend(sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:27:19.585988Z","iopub.status.idle":"2025-07-26T08:27:19.586392Z","shell.execute_reply.started":"2025-07-26T08:27:19.586190Z","shell.execute_reply":"2025-07-26T08:27:19.586206Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nclass GPTChunkedDataset(torch.utils.data.Dataset):\n    def __init__(self, token_ids, block_size):\n        self.token_ids = token_ids\n        self.block_size = block_size\n\n        # How many full non-overlapping blocks fit?\n        self.num_chunks = (len(self.token_ids) - 1) // block_size\n\n    def __len__(self):\n        return self.num_chunks\n\n    def __getitem__(self, idx):\n        start = idx * self.block_size\n        x = torch.tensor(self.token_ids[start : start + self.block_size], dtype=torch.long)\n        y = torch.tensor(self.token_ids[start + 1 : start + 1 + self.block_size], dtype=torch.long)\n        return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:04.944132Z","iopub.execute_input":"2025-07-26T08:34:04.944699Z","iopub.status.idle":"2025-07-26T08:34:04.950595Z","shell.execute_reply.started":"2025-07-26T08:34:04.944675Z","shell.execute_reply":"2025-07-26T08:34:04.949647Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset = GPTChunkedDataset(encoded_dataset, block_size=64)\n\nloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=1024,\n    shuffle=True,   # Shuffle for training, optional\n    drop_last=True,  # Drop partial batch\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:20.368091Z","iopub.execute_input":"2025-07-26T08:34:20.368424Z","iopub.status.idle":"2025-07-26T08:34:20.443649Z","shell.execute_reply.started":"2025-07-26T08:34:20.368371Z","shell.execute_reply":"2025-07-26T08:34:20.442822Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"torch.cuda.device_count()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:23.115004Z","iopub.execute_input":"2025-07-26T08:34:23.115323Z","iopub.status.idle":"2025-07-26T08:34:23.155494Z","shell.execute_reply.started":"2025-07-26T08:34:23.115301Z","shell.execute_reply":"2025-07-26T08:34:23.154817Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"for xb, yb in loader:\n    print(xb.shape)  # (256, 64)     sequence_length=64\n    print(yb.shape)  # (256, 64)     sequence_length=64\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:56:37.838320Z","iopub.status.idle":"2025-07-25T14:56:37.838666Z","shell.execute_reply.started":"2025-07-25T14:56:37.838485Z","shell.execute_reply":"2025-07-25T14:56:37.838499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TokenAndPositionEmbedding(nn.Module):\n    def __init__(self, vocab_size=10000, embed_dim=256, max_len=200):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n        self.position_emb = nn.Embedding(max_len, embed_dim)\n\n    def forward(self, x):\n        batch_size, seq_len = x.size()\n        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n        token_embedding = self.token_emb(x)\n        position_embedding = self.position_emb(positions)\n        return token_embedding + position_embedding           #torch.Size([256, 64, 128])    b,s,e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:30.664120Z","iopub.execute_input":"2025-07-26T08:34:30.664676Z","iopub.status.idle":"2025-07-26T08:34:30.670316Z","shell.execute_reply.started":"2025-07-26T08:34:30.664653Z","shell.execute_reply":"2025-07-26T08:34:30.669448Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"tpe = TokenAndPositionEmbedding(\n    vocab_size=4000,\n    embed_dim=256,\n    max_len=200\n)\n\nout = tpe(xb)\nout.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T11:54:22.201934Z","iopub.execute_input":"2025-07-24T11:54:22.202476Z","iopub.status.idle":"2025-07-24T11:54:22.317214Z","shell.execute_reply.started":"2025-07-24T11:54:22.202455Z","shell.execute_reply":"2025-07-24T11:54:22.316451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim=None, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        \n        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n        self.ln1 = nn.LayerNorm(embed_dim)\n\n        ff_hidden_dim = ff_hidden_dim or embed_dim * 4\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, ff_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(ff_hidden_dim, embed_dim)\n        )\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, seq_len, embed_dim)\n        \"\"\"\n        batch_size, seq_len, _ = x.size()\n\n        # Transpose for MultiheadAttention: (seq_len, batch_size, embed_dim)\n        x_t = x.transpose(0, 1)\n\n        # Make causal mask: shape (seq_len, seq_len)\n        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n\n        # Masked self-attention\n        attn_out, _ = self.attn(x_t, x_t, x_t, attn_mask=attn_mask)\n\n        # Residual + LayerNorm\n        x2 = self.ln1(x_t + self.dropout(attn_out))\n\n        # Feed Forward\n        ff_out = self.ff(x2)\n\n        # Residual + LayerNorm\n        out = self.ln2(x2 + self.dropout(ff_out))\n\n        # Transpose back: (batch_size, seq_len, embed_dim)\n        return out.transpose(0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:34.614514Z","iopub.execute_input":"2025-07-26T08:34:34.615169Z","iopub.status.idle":"2025-07-26T08:34:34.623109Z","shell.execute_reply.started":"2025-07-26T08:34:34.615145Z","shell.execute_reply":"2025-07-26T08:34:34.622357Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass MiniGPTBlock(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, num_heads=8, num_layers=2, max_len=128, dropout=0.1):\n        super().__init__()\n        self.embedding = TokenAndPositionEmbedding(vocab_size, embed_dim, max_len)\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, dropout=dropout) for _ in range(num_layers)\n        ])\n        self.ln_f = nn.LayerNorm(embed_dim)\n        self.lm_head = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, seq_len)\n        returns logits: (batch_size, seq_len, vocab_size)\n        \"\"\"\n        x = self.embedding(x)\n        for block in self.blocks:\n            x = block(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:36.691255Z","iopub.execute_input":"2025-07-26T08:34:36.692242Z","iopub.status.idle":"2025-07-26T08:34:36.698556Z","shell.execute_reply.started":"2025-07-26T08:34:36.692213Z","shell.execute_reply":"2025-07-26T08:34:36.697829Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = MiniGPTBlock(vocab_size=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:39.206673Z","iopub.execute_input":"2025-07-26T08:34:39.206989Z","iopub.status.idle":"2025-07-26T08:34:39.730690Z","shell.execute_reply.started":"2025-07-26T08:34:39.206967Z","shell.execute_reply":"2025-07-26T08:34:39.729785Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:41.099970Z","iopub.execute_input":"2025-07-26T08:34:41.100265Z","iopub.status.idle":"2025-07-26T08:34:41.169134Z","shell.execute_reply.started":"2025-07-26T08:34:41.100243Z","shell.execute_reply":"2025-07-26T08:34:41.168450Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"device","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:41.454539Z","iopub.execute_input":"2025-07-26T08:34:41.455566Z","iopub.status.idle":"2025-07-26T08:34:41.461452Z","shell.execute_reply.started":"2025-07-26T08:34:41.455530Z","shell.execute_reply":"2025-07-26T08:34:41.460617Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:34:46.018927Z","iopub.execute_input":"2025-07-26T08:34:46.019187Z","iopub.status.idle":"2025-07-26T08:34:46.197498Z","shell.execute_reply.started":"2025-07-26T08:34:46.019169Z","shell.execute_reply":"2025-07-26T08:34:46.196803Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"MiniGPTBlock(\n  (embedding): TokenAndPositionEmbedding(\n    (token_emb): Embedding(10000, 256)\n    (position_emb): Embedding(128, 256)\n  )\n  (blocks): ModuleList(\n    (0-1): 2 x TransformerBlock(\n      (attn): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n      )\n      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (ff): Sequential(\n        (0): Linear(in_features=256, out_features=1024, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=1024, out_features=256, bias=True)\n      )\n      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (lm_head): Linear(in_features=256, out_features=10000, bias=True)\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"epochs = 50\nlearning_rate = 0.002\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:34:42.340408Z","iopub.execute_input":"2025-07-22T10:34:42.340930Z","iopub.status.idle":"2025-07-22T10:34:42.345429Z","shell.execute_reply.started":"2025-07-22T10:34:42.340904Z","shell.execute_reply":"2025-07-22T10:34:42.344845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(epochs):\n  total_loss = 0\n\n  for batch_x, batch_y in loader:\n\n    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n    optimizer.zero_grad()\n\n    logits = model(batch_x)  # (batch, seq_len, vocab_size)\n\n    loss = criterion(\n      logits.view(-1, vocab_size),\n      batch_y.view(-1)\n    )\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n    optimizer.step()\n\n    total_loss += loss.item()\n\n  print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:34:49.960453Z","iopub.execute_input":"2025-07-22T10:34:49.960949Z","iopub.status.idle":"2025-07-22T10:42:02.529157Z","shell.execute_reply.started":"2025-07-22T10:34:49.960922Z","shell.execute_reply":"2025-07-22T10:42:02.528338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nepochs = 20\nmodel.train()\nscaler = GradScaler(device='cuda')\nvocab_size = 10000\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, betas=(0.9, 0.98), eps=1e-9)\nscheduler = OneCycleLR(optimizer, max_lr=2e-3, total_steps=epochs*len(loader))\n\n\nfor epoch in range(epochs):\n    total_loss = 0\n    for batch_x, batch_y in loader:\n        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n        optimizer.zero_grad(set_to_none=True)\n        with autocast(device_type='cuda'):\n            logits = model(batch_x)\n            loss = criterion(logits.view(-1, vocab_size), batch_y.view(-1))\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        scaler.step(optimizer)\n        scaler.update()\n        scheduler.step()\n        total_loss += loss.item()\n    print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T08:37:29.966685Z","iopub.execute_input":"2025-07-26T08:37:29.967008Z","iopub.status.idle":"2025-07-26T10:41:56.041642Z","shell.execute_reply.started":"2025-07-26T08:37:29.966985Z","shell.execute_reply":"2025-07-26T10:41:56.040851Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1, Loss: 4296.2400\nEpoch: 2, Loss: 3129.8721\nEpoch: 3, Loss: 2720.2601\nEpoch: 4, Loss: 2510.4969\nEpoch: 5, Loss: 2402.4811\nEpoch: 6, Loss: 2335.1439\nEpoch: 7, Loss: 2285.8471\nEpoch: 8, Loss: 2249.1743\nEpoch: 9, Loss: 2219.8334\nEpoch: 10, Loss: 2194.9839\nEpoch: 11, Loss: 2172.8641\nEpoch: 12, Loss: 2152.7044\nEpoch: 13, Loss: 2133.6338\nEpoch: 14, Loss: 2116.1039\nEpoch: 15, Loss: 2099.8382\nEpoch: 16, Loss: 2085.4197\nEpoch: 17, Loss: 2072.8524\nEpoch: 18, Loss: 2062.9057\nEpoch: 19, Loss: 2055.7569\nEpoch: 20, Loss: 2051.8960\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"torch.save(model.state_dict(), 'gpt_from_scratch_in_pytorch_on_stories_generation_300k.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T10:41:57.212004Z","iopub.execute_input":"2025-07-26T10:41:57.212322Z","iopub.status.idle":"2025-07-26T10:41:57.301327Z","shell.execute_reply.started":"2025-07-26T10:41:57.212305Z","shell.execute_reply":"2025-07-26T10:41:57.300382Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import torch\n\ndef generate(\n    model,\n    tokenizer,           # your tokenizer to decode IDs to text\n    device,\n    start_tokens,        # list of ints, e.g. [BOS] or your prompt encoded\n    max_new_tokens=50,   # how many tokens to generate\n    block_size=64,       # context window\n    temperature=1.0,     # controls randomness\n    top_k=None           # optional: restrict to top-k for more randomness control\n):\n    model.eval()\n    generated = start_tokens.copy()\n    input_ids = torch.tensor(start_tokens, dtype=torch.long).unsqueeze(0).to(device)  # (1, len)\n\n    for _ in range(max_new_tokens):\n        # If context is longer than block_size, crop oldest tokens\n        if input_ids.size(1) > block_size:\n            input_ids = input_ids[:, -block_size:]\n\n        with torch.no_grad():\n            logits = model(input_ids)  # (1, seq_len, vocab_size)\n\n        # Get logits for last token only\n        logits = logits[:, -1, :] / temperature  # (1, vocab_size)\n\n        # Optionally top-k filter\n        if top_k is not None:\n            v, ix = torch.topk(logits, top_k)\n            logits[logits < v[0, -1]] = -float('Inf')\n\n        probs = torch.softmax(logits, dim=-1)  # (1, vocab_size)\n\n        next_token = torch.multinomial(probs, num_samples=1)  # (1, 1)\n\n        next_token_id = next_token.item()\n\n        # Append to sequence\n        generated.append(next_token_id)\n\n        # Update input_ids\n        input_ids = torch.cat([input_ids, next_token], dim=1)  # (1, seq_len+1)\n\n\n    return generated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T10:41:57.302342Z","iopub.execute_input":"2025-07-26T10:41:57.302606Z","iopub.status.idle":"2025-07-26T10:41:57.310167Z","shell.execute_reply.started":"2025-07-26T10:41:57.302585Z","shell.execute_reply":"2025-07-26T10:41:57.309273Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# Suppose your tokenizer encodes/decodes:\nstart_text = \"Once upon a time, there was a boy named tim\"\nstart_tokens = tokenizer.encode(start_text) # Or whatever you used\n\noutput_tokens = generate(\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    start_tokens=start_tokens,\n    max_new_tokens=250,\n    block_size=64,\n    temperature=0.4,\n    top_k=50  # try top_k for more diverse output\n)\n\n# Decode\noutput_text = tokenizer.decode(output_tokens)\nprint(output_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T11:18:58.974364Z","iopub.execute_input":"2025-07-26T11:18:58.974688Z","iopub.status.idle":"2025-07-26T11:18:59.470252Z","shell.execute_reply.started":"2025-07-26T11:18:58.974667Z","shell.execute_reply":"2025-07-26T11:18:59.469370Z"}},"outputs":[{"name":"stdout","text":"Once upon a time, there was a boy named timet. He was very curious and loved to explore. One day, he went to the park with his mom. He saw a big tree and wanted to climb it. He asked his mom if he could climb it, but she said no. The boy was sad and he started to cry.\n\nHis mom told him that he could not climb the tree. She said that if he was careful, he should not climb the tree.\n\nThe boy was sad, but he knew his mom was right. So he decided to try again. He climbed the tree and found a big, juicy apple. He was so happy!\n\nThe boy climbed the tree and took a big bite. He was so happy to have found the apple. He ate it all up and was no longer hungry.\n\nThe boy was so happy that he had found something so yummy. He ate it up and he felt full and happy. He knew he had made a new friend.Once upon a time there was a little girl named Lucy. She was three years old and loved to play outside. One day, Lucy was playing in the garden when she saw a big, scary dog. The dog was barking and running towards her. Lucy was scared,\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"print(\"done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-26T10:41:57.206132Z","iopub.execute_input":"2025-07-26T10:41:57.206901Z","iopub.status.idle":"2025-07-26T10:41:57.211139Z","shell.execute_reply.started":"2025-07-26T10:41:57.206873Z","shell.execute_reply":"2025-07-26T10:41:57.210354Z"}},"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"tokenizer.save(\"my_tokenizer.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:17:10.997330Z","iopub.execute_input":"2025-07-22T10:17:10.997938Z","iopub.status.idle":"2025-07-22T10:17:11.006709Z","shell.execute_reply.started":"2025-07-22T10:17:10.997913Z","shell.execute_reply":"2025-07-22T10:17:11.005980Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}