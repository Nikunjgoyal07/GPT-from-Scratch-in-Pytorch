{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:54:30.276935Z","iopub.execute_input":"2025-07-22T08:54:30.277215Z","iopub.status.idle":"2025-07-22T08:54:32.470200Z","shell.execute_reply.started":"2025-07-22T08:54:30.277188Z","shell.execute_reply":"2025-07-22T08:54:32.469344Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"#vocab_size = 4000\n#max_len_list = 1119","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:34:15.944676Z","iopub.execute_input":"2025-07-22T09:34:15.944929Z","iopub.status.idle":"2025-07-22T09:34:15.948708Z","shell.execute_reply.started":"2025-07-22T09:34:15.944913Z","shell.execute_reply":"2025-07-22T09:34:15.947901Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:10:37.711366Z","iopub.execute_input":"2025-07-22T09:10:37.711932Z","iopub.status.idle":"2025-07-22T09:10:42.294357Z","shell.execute_reply.started":"2025-07-22T09:10:37.711913Z","shell.execute_reply":"2025-07-22T09:10:42.293629Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"roneneldan/TinyStories\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:23:06.015199Z","iopub.execute_input":"2025-07-22T09:23:06.015867Z","iopub.status.idle":"2025-07-22T09:23:07.294073Z","shell.execute_reply.started":"2025-07-22T09:23:06.015835Z","shell.execute_reply":"2025-07-22T09:23:07.293376Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"len(dataset[\"validation\"]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:25:17.790007Z","iopub.execute_input":"2025-07-22T09:25:17.790669Z","iopub.status.idle":"2025-07-22T09:25:17.826739Z","shell.execute_reply.started":"2025-07-22T09:25:17.790645Z","shell.execute_reply":"2025-07-22T09:25:17.826126Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"21990"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"dataset = dataset[\"validation\"]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:25:52.367904Z","iopub.execute_input":"2025-07-22T09:25:52.368206Z","iopub.status.idle":"2025-07-22T09:25:52.400560Z","shell.execute_reply.started":"2025-07-22T09:25:52.368183Z","shell.execute_reply":"2025-07-22T09:25:52.399832Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"dataset2 = []\nfor i in dataset:\n    if len(i) > 100:\n        dataset2.append(i)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:25:59.975532Z","iopub.execute_input":"2025-07-22T09:25:59.975846Z","iopub.status.idle":"2025-07-22T09:26:00.277308Z","shell.execute_reply.started":"2025-07-22T09:25:59.975824Z","shell.execute_reply":"2025-07-22T09:26:00.276628Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"len(dataset2)\ndataset2[:2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:26:52.446101Z","iopub.execute_input":"2025-07-22T09:26:52.446394Z","iopub.status.idle":"2025-07-22T09:26:52.451319Z","shell.execute_reply.started":"2025-07-22T09:26:52.446372Z","shell.execute_reply":"2025-07-22T09:26:52.450786Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every day.\"\\n\\nAfter playing with the car, Kitty and Spot felt thirsty. They found a small pond with clear water. They drank the water and felt very happy. They played together all day and became best friends.',\n 'Once upon a time, in a big forest, there lived a rhinoceros named Roxy. Roxy loved to climb. She climbed trees, rocks, and hills. One day, Roxy found an icy hill. She had never seen anything like it before. It was shiny and cold, and she wanted to climb it.\\n\\nRoxy tried to climb the icy hill, but it was very slippery. She tried again and again, but she kept falling down. Roxy was sad. She wanted to climb the icy hill so much. Then, she saw a little bird named Billy. Billy saw that Roxy was sad and asked, \"Why are you sad, Roxy?\"\\n\\nRoxy told Billy about the icy hill and how she couldn\\'t climb it. Billy said, \"I have an idea! Let\\'s find some big leaves to put under your feet. They will help you climb the icy hill.\" Roxy and Billy looked for big leaves and found some. Roxy put the leaves under her feet and tried to climb the icy hill again.\\n\\nThis time, Roxy didn\\'t slip. She climbed and climbed until she reached the top of the icy hill. Roxy was so happy! She and Billy played on the icy hill all day. From that day on, Roxy and Billy were the best of friends, and they climbed and played together all the time. And Roxy learned that with a little help from a friend, she could climb anything.']"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"!pip install tokenizers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:13:21.835446Z","iopub.execute_input":"2025-07-22T09:13:21.836080Z","iopub.status.idle":"2025-07-22T09:13:25.157171Z","shell.execute_reply.started":"2025-07-22T09:13:21.836055Z","shell.execute_reply":"2025-07-22T09:13:25.156424Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.3.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.6.15)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n\n# 1. Make a BPE tokenizer\ntokenizer = Tokenizer(models.BPE())\n\n# 2. Pre-tokenizer: ByteLevel is common for GPT-like BPE\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\n# 3. Decoder: ByteLevel to decode back\ntokenizer.decoder = decoders.ByteLevel()\n\n# 4. Trainer: define vocab size, min frequency, special tokens\ntrainer = trainers.BpeTrainer(\n    vocab_size=4000,  # adjust as needed\n    min_frequency=2,\n    special_tokens=[\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n)\n\n# 5. Train on your list of strings\ntokenizer.train_from_iterator(dataset2, trainer)\n\n# 6. Save the tokenizer if you want\ntokenizer.save(\"bpe_tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:26:25.144130Z","iopub.execute_input":"2025-07-22T09:26:25.144688Z","iopub.status.idle":"2025-07-22T09:26:28.483826Z","shell.execute_reply.started":"2025-07-22T09:26:25.144665Z","shell.execute_reply":"2025-07-22T09:26:28.483175Z"}},"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(\"Actual vocab size:\", tokenizer.get_vocab_size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:26:31.200423Z","iopub.execute_input":"2025-07-22T09:26:31.201068Z","iopub.status.idle":"2025-07-22T09:26:31.207727Z","shell.execute_reply.started":"2025-07-22T09:26:31.201043Z","shell.execute_reply":"2025-07-22T09:26:31.206998Z"}},"outputs":[{"name":"stdout","text":"Actual vocab size: 4000\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"for sentence in dataset2:\n    encoded = tokenizer.encode(sentence)\n    print(f\"Sentence: {sentence}\")\n    print(f\"Tokens: {encoded.tokens}\")\n    print(f\"IDs: {encoded.ids}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoded = tokenizer.encode(dataset2[0])\nprint(f\"Sentence: {dataset2[0]}\")\nprint(f\"Tokens: {encoded.tokens}\")\nprint(f\"IDs: {encoded.ids}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:29:34.883394Z","iopub.execute_input":"2025-07-22T09:29:34.883694Z","iopub.status.idle":"2025-07-22T09:29:34.889060Z","shell.execute_reply.started":"2025-07-22T09:29:34.883673Z","shell.execute_reply":"2025-07-22T09:29:34.888292Z"}},"outputs":[{"name":"stdout","text":"Sentence: Spot. Spot saw the shiny car and said, \"Wow, Kitty, your car is so bright and clean!\" Kitty smiled and replied, \"Thank you, Spot. I polish it every day.\"\n\nAfter playing with the car, Kitty and Spot felt thirsty. They found a small pond with clear water. They drank the water and felt very happy. They played together all day and became best friends.\nTokens: ['ĠSpot', '.', 'ĠSpot', 'Ġsaw', 'Ġthe', 'Ġshiny', 'Ġcar', 'Ġand', 'Ġsaid', ',', 'Ġ\"', 'Wow', ',', 'ĠKitty', ',', 'Ġyour', 'Ġcar', 'Ġis', 'Ġso', 'Ġbright', 'Ġand', 'Ġclean', '!\"', 'ĠKitty', 'Ġsmiled', 'Ġand', 'Ġreplied', ',', 'Ġ\"', 'Thank', 'Ġyou', ',', 'ĠSpot', '.', 'ĠI', 'Ġpol', 'ish', 'Ġit', 'Ġevery', 'Ġday', '.\"', 'Ċ', 'Ċ', 'After', 'Ġplaying', 'Ġwith', 'Ġthe', 'Ġcar', ',', 'ĠKitty', 'Ġand', 'ĠSpot', 'Ġfelt', 'Ġthirsty', '.', 'ĠThey', 'Ġfound', 'Ġa', 'Ġsmall', 'Ġpond', 'Ġwith', 'Ġclear', 'Ġwater', '.', 'ĠThey', 'Ġdrank', 'Ġthe', 'Ġwater', 'Ġand', 'Ġfelt', 'Ġvery', 'Ġhappy', '.', 'ĠThey', 'Ġplayed', 'Ġtogether', 'Ġall', 'Ġday', 'Ġand', 'Ġbecame', 'Ġbest', 'Ġfriends', '.']\nIDs: [999, 16, 999, 264, 116, 823, 429, 118, 180, 14, 189, 1215, 14, 2456, 14, 508, 429, 275, 200, 966, 118, 796, 289, 2456, 359, 118, 929, 14, 189, 906, 209, 14, 999, 16, 194, 1434, 572, 157, 472, 207, 278, 99, 99, 1029, 462, 195, 116, 429, 14, 2456, 118, 999, 357, 3421, 16, 177, 434, 112, 647, 1144, 195, 2917, 615, 16, 177, 2776, 116, 615, 118, 357, 241, 258, 16, 177, 609, 414, 327, 207, 118, 955, 694, 335, 16]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"encoded_dataset = []\nfor sentence in dataset2:\n    encoded = tokenizer.encode(sentence)\n    encoded_dataset.append(encoded.ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:31:14.079223Z","iopub.execute_input":"2025-07-22T09:31:14.079497Z","iopub.status.idle":"2025-07-22T09:31:30.216201Z","shell.execute_reply.started":"2025-07-22T09:31:14.079477Z","shell.execute_reply":"2025-07-22T09:31:30.215401Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"max_len_list =0\n\nfor sequence in encoded_dataset:\n  if len(sequence) >= max_len_list:\n      max_len_list= len(sequence)\n\nmax_len_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:33:55.686323Z","iopub.execute_input":"2025-07-22T09:33:55.686920Z","iopub.status.idle":"2025-07-22T09:33:55.693951Z","shell.execute_reply.started":"2025-07-22T09:33:55.686895Z","shell.execute_reply":"2025-07-22T09:33:55.693384Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"1119"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"print(encoded_dataset[0:2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:39:30.233934Z","iopub.execute_input":"2025-07-22T09:39:30.234726Z","iopub.status.idle":"2025-07-22T09:39:30.238748Z","shell.execute_reply.started":"2025-07-22T09:39:30.234699Z","shell.execute_reply":"2025-07-22T09:39:30.237887Z"}},"outputs":[{"name":"stdout","text":"[[999, 16, 999, 264, 116, 823, 429, 118, 180, 14, 189, 1215, 14, 2456, 14, 508, 429, 275, 200, 966, 118, 796, 289, 2456, 359, 118, 929, 14, 189, 906, 209, 14, 999, 16, 194, 1434, 572, 157, 472, 207, 278, 99, 99, 1029, 462, 195, 116, 429, 14, 2456, 118, 999, 357, 3421, 16, 177, 434, 112, 647, 1144, 195, 2917, 615, 16, 177, 2776, 116, 615, 118, 357, 241, 258, 16, 177, 609, 414, 327, 207, 118, 955, 694, 335, 16], [276, 292, 112, 242, 14, 170, 112, 260, 891, 14, 245, 762, 112, 3831, 341, 962, 72, 3817, 16, 962, 72, 3817, 348, 119, 860, 16, 166, 1389, 1204, 14, 1800, 14, 118, 1274, 76, 16, 328, 207, 14, 962, 72, 3817, 434, 267, 3164, 1274, 16, 166, 214, 599, 1160, 1293, 378, 157, 829, 16, 265, 133, 823, 118, 1227, 14, 118, 188, 295, 119, 860, 157, 16, 99, 99, 49, 72, 3817, 582, 119, 860, 116, 3164, 1274, 14, 261, 157, 133, 241, 568, 2821, 82, 16, 166, 582, 437, 118, 437, 14, 261, 188, 781, 3459, 480, 16, 962, 72, 3817, 133, 421, 16, 166, 295, 119, 860, 116, 3164, 1274, 200, 564, 16, 728, 14, 188, 264, 112, 244, 387, 341, 1321, 16, 1321, 264, 210, 962, 72, 3817, 133, 421, 118, 361, 14, 189, 1693, 314, 209, 421, 14, 962, 72, 3817, 345, 99, 99, 49, 72, 3817, 597, 1321, 518, 116, 3164, 1274, 118, 561, 188, 574, 273, 860, 157, 16, 1321, 180, 14, 189, 40, 324, 267, 828, 4, 1129, 226, 487, 370, 260, 1379, 119, 422, 734, 508, 2200, 16, 177, 548, 333, 209, 860, 116, 3164, 1274, 278, 962, 72, 3817, 118, 1321, 371, 221, 260, 1379, 118, 434, 370, 16, 962, 72, 3817, 422, 116, 1379, 734, 162, 2200, 118, 582, 119, 860, 116, 3164, 1274, 437, 16, 99, 99, 1413, 242, 14, 962, 72, 3817, 494, 273, 568, 789, 16, 166, 1389, 118, 1389, 650, 188, 1087, 116, 1172, 199, 116, 3164, 1274, 16, 962, 72, 3817, 133, 200, 258, 4, 166, 118, 1321, 609, 198, 116, 3164, 1274, 327, 207, 16, 736, 210, 207, 198, 14, 962, 72, 3817, 118, 1321, 279, 116, 694, 199, 335, 14, 118, 217, 1389, 118, 609, 414, 327, 116, 242, 16, 730, 962, 72, 3817, 659, 210, 195, 112, 244, 333, 424, 112, 270, 14, 188, 305, 860, 1293, 16]]\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"merged_dataset = []\nfor sentence in encoded_dataset:\n    merged_dataset.extend(sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:43:31.055710Z","iopub.execute_input":"2025-07-22T09:43:31.055976Z","iopub.status.idle":"2025-07-22T09:43:31.131435Z","shell.execute_reply.started":"2025-07-22T09:43:31.055955Z","shell.execute_reply":"2025-07-22T09:43:31.130680Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"import torch\n\nclass GPTChunkedDataset(torch.utils.data.Dataset):\n    def __init__(self, token_ids, block_size):\n        self.token_ids = token_ids\n        self.block_size = block_size\n\n        # How many full non-overlapping blocks fit?\n        self.num_chunks = (len(self.token_ids) - 1) // block_size\n\n    def __len__(self):\n        return self.num_chunks\n\n    def __getitem__(self, idx):\n        start = idx * self.block_size\n        x = torch.tensor(self.token_ids[start : start + self.block_size], dtype=torch.long)\n        y = torch.tensor(self.token_ids[start + 1 : start + 1 + self.block_size], dtype=torch.long)\n        return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:48:58.381008Z","iopub.execute_input":"2025-07-22T09:48:58.381539Z","iopub.status.idle":"2025-07-22T09:49:04.339729Z","shell.execute_reply.started":"2025-07-22T09:48:58.381516Z","shell.execute_reply":"2025-07-22T09:49:04.339073Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"dataset = GPTChunkedDataset(merged_dataset, block_size=64)\n\nloader = torch.utils.data.DataLoader(\n    dataset,\n    batch_size=256,\n    shuffle=True,   # Shuffle for training, optional\n    drop_last=True,  # Drop partial batch\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:55:56.503317Z","iopub.execute_input":"2025-07-22T09:55:56.503641Z","iopub.status.idle":"2025-07-22T09:55:56.507950Z","shell.execute_reply.started":"2025-07-22T09:55:56.503610Z","shell.execute_reply":"2025-07-22T09:55:56.507140Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"for xb, yb in loader:\n    print(xb.shape)  # (256, 64)     sequence_length=64\n    print(yb.shape)  # (256, 64)     sequence_length=64\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:55:56.922609Z","iopub.execute_input":"2025-07-22T09:55:56.922893Z","iopub.status.idle":"2025-07-22T09:55:56.939779Z","shell.execute_reply.started":"2025-07-22T09:55:56.922872Z","shell.execute_reply":"2025-07-22T09:55:56.939045Z"}},"outputs":[{"name":"stdout","text":"torch.Size([256, 64])\ntorch.Size([256, 64])\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TokenAndPositionEmbedding(nn.Module):\n    def __init__(self, vocab_size=4000, embed_dim=128, max_len=200):\n        super().__init__()\n        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n        self.position_emb = nn.Embedding(max_len, embed_dim)\n\n    def forward(self, x):\n        batch_size, seq_len = x.size()\n        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n        token_embedding = self.token_emb(x)\n        position_embedding = self.position_emb(positions)\n        return token_embedding + position_embedding           #torch.Size([256, 64, 128])    b,s,e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:55:59.149948Z","iopub.execute_input":"2025-07-22T09:55:59.150697Z","iopub.status.idle":"2025-07-22T09:55:59.155781Z","shell.execute_reply.started":"2025-07-22T09:55:59.150666Z","shell.execute_reply":"2025-07-22T09:55:59.155118Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"tpe = TokenAndPositionEmbedding(\n    vocab_size=4000,\n    embed_dim=128,\n    max_len=200\n)\n\nout = tpe(xb)\nout.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:55:59.470360Z","iopub.execute_input":"2025-07-22T09:55:59.470663Z","iopub.status.idle":"2025-07-22T09:55:59.486253Z","shell.execute_reply.started":"2025-07-22T09:55:59.470639Z","shell.execute_reply":"2025-07-22T09:55:59.485635Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"torch.Size([256, 64, 128])"},"metadata":{}}],"execution_count":76},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_hidden_dim=None, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        \n        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n        self.ln1 = nn.LayerNorm(embed_dim)\n\n        ff_hidden_dim = ff_hidden_dim or embed_dim * 4\n        self.ff = nn.Sequential(\n            nn.Linear(embed_dim, ff_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(ff_hidden_dim, embed_dim)\n        )\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        x: (batch_size, seq_len, embed_dim)\n        \"\"\"\n        batch_size, seq_len, _ = x.size()\n\n        # Transpose for MultiheadAttention: (seq_len, batch_size, embed_dim)\n        x_t = x.transpose(0, 1)\n\n        # Make causal mask: shape (seq_len, seq_len)\n        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), diagonal=1).bool()\n\n        # Masked self-attention\n        attn_out, _ = self.attn(x_t, x_t, x_t, attn_mask=attn_mask)\n\n        # Residual + LayerNorm\n        x2 = self.ln1(x_t + self.dropout(attn_out))\n\n        # Feed Forward\n        ff_out = self.ff(x2)\n\n        # Residual + LayerNorm\n        out = self.ln2(x2 + self.dropout(ff_out))\n\n        # Transpose back: (batch_size, seq_len, embed_dim)\n        return out.transpose(0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:56:23.764631Z","iopub.execute_input":"2025-07-22T09:56:23.764919Z","iopub.status.idle":"2025-07-22T09:56:23.772240Z","shell.execute_reply.started":"2025-07-22T09:56:23.764898Z","shell.execute_reply":"2025-07-22T09:56:23.771450Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass MiniGPTBlock(nn.Module):\n    def __init__(self, vocab_size, embed_dim=128, num_heads=8, max_len=66):\n        super().__init__()\n        self.embedding = TokenAndPositionEmbedding(vocab_size, embed_dim, max_len)\n        self.transformer = TransformerBlock(embed_dim, num_heads)\n        self.lm_head = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, x):\n        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n        x = self.transformer(x)  # (batch_size, seq_len, embed_dim)\n        logits = self.lm_head(x)  # (batch_size, vocab_size)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:58:08.755154Z","iopub.execute_input":"2025-07-22T09:58:08.755912Z","iopub.status.idle":"2025-07-22T09:58:08.761011Z","shell.execute_reply.started":"2025-07-22T09:58:08.755884Z","shell.execute_reply":"2025-07-22T09:58:08.760197Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"model = MiniGPTBlock(vocab_size=4000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:58:49.899250Z","iopub.execute_input":"2025-07-22T09:58:49.899521Z","iopub.status.idle":"2025-07-22T09:58:49.914275Z","shell.execute_reply.started":"2025-07-22T09:58:49.899502Z","shell.execute_reply":"2025-07-22T09:58:49.913686Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:58:57.748299Z","iopub.execute_input":"2025-07-22T09:58:57.748852Z","iopub.status.idle":"2025-07-22T09:58:57.802484Z","shell.execute_reply.started":"2025-07-22T09:58:57.748828Z","shell.execute_reply":"2025-07-22T09:58:57.801790Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:59:07.126813Z","iopub.execute_input":"2025-07-22T09:59:07.127085Z","iopub.status.idle":"2025-07-22T09:59:07.327785Z","shell.execute_reply.started":"2025-07-22T09:59:07.127065Z","shell.execute_reply":"2025-07-22T09:59:07.327128Z"}},"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"MiniGPTBlock(\n  (embedding): TokenAndPositionEmbedding(\n    (token_emb): Embedding(4000, 128)\n    (position_emb): Embedding(66, 128)\n  )\n  (transformer): TransformerBlock(\n    (attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n    )\n    (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (ff): Sequential(\n      (0): Linear(in_features=128, out_features=512, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=512, out_features=128, bias=True)\n    )\n    (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=128, out_features=4000, bias=True)\n)"},"metadata":{}}],"execution_count":81},{"cell_type":"code","source":"epochs = 50\nlearning_rate = 0.005\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T09:59:56.823024Z","iopub.execute_input":"2025-07-22T09:59:56.823316Z","iopub.status.idle":"2025-07-22T10:00:01.951296Z","shell.execute_reply.started":"2025-07-22T09:59:56.823292Z","shell.execute_reply":"2025-07-22T10:00:01.950750Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"for epoch in range(epochs):\n  total_loss = 0\n\n  for batch_x, batch_y in loader:\n\n    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n    optimizer.zero_grad()\n\n    logits = model(batch_x)  # (batch, seq_len, vocab_size)\n\n    loss = criterion(\n      logits.view(-1, vocab_size),\n      batch_y.view(-1)\n    )\n\n    loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n    optimizer.step()\n\n    total_loss += loss.item()\n\n  print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:02:32.093467Z","iopub.execute_input":"2025-07-22T10:02:32.093763Z","iopub.status.idle":"2025-07-22T10:09:56.469068Z","shell.execute_reply.started":"2025-07-22T10:02:32.093739Z","shell.execute_reply":"2025-07-22T10:09:56.468216Z"}},"outputs":[{"name":"stdout","text":"Epoch: 1, Loss: 1172.3973\nEpoch: 2, Loss: 928.6028\nEpoch: 3, Loss: 877.1469\nEpoch: 4, Loss: 849.3757\nEpoch: 5, Loss: 830.2931\nEpoch: 6, Loss: 816.7812\nEpoch: 7, Loss: 806.2519\nEpoch: 8, Loss: 798.2785\nEpoch: 9, Loss: 791.2130\nEpoch: 10, Loss: 785.5917\nEpoch: 11, Loss: 780.6116\nEpoch: 12, Loss: 776.4056\nEpoch: 13, Loss: 772.6126\nEpoch: 14, Loss: 769.3013\nEpoch: 15, Loss: 766.1249\nEpoch: 16, Loss: 763.4112\nEpoch: 17, Loss: 760.9264\nEpoch: 18, Loss: 758.5742\nEpoch: 19, Loss: 756.5006\nEpoch: 20, Loss: 754.5983\nEpoch: 21, Loss: 752.7565\nEpoch: 22, Loss: 750.9802\nEpoch: 23, Loss: 749.4719\nEpoch: 24, Loss: 747.8123\nEpoch: 25, Loss: 746.4548\nEpoch: 26, Loss: 745.2027\nEpoch: 27, Loss: 743.8756\nEpoch: 28, Loss: 742.6226\nEpoch: 29, Loss: 741.5002\nEpoch: 30, Loss: 740.5686\nEpoch: 31, Loss: 739.2706\nEpoch: 32, Loss: 738.3365\nEpoch: 33, Loss: 737.4951\nEpoch: 34, Loss: 736.6225\nEpoch: 35, Loss: 735.8308\nEpoch: 36, Loss: 734.8306\nEpoch: 37, Loss: 734.2506\nEpoch: 38, Loss: 733.3519\nEpoch: 39, Loss: 732.7443\nEpoch: 40, Loss: 732.0504\nEpoch: 41, Loss: 731.2390\nEpoch: 42, Loss: 730.6366\nEpoch: 43, Loss: 730.0940\nEpoch: 44, Loss: 729.5377\nEpoch: 45, Loss: 728.9265\nEpoch: 46, Loss: 728.5217\nEpoch: 47, Loss: 727.8171\nEpoch: 48, Loss: 727.3498\nEpoch: 49, Loss: 726.8514\nEpoch: 50, Loss: 726.4159\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"torch.save(model.state_dict(), 'gpt_from_scratch_in_pytorch_on_stories_generation.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:09:56.470180Z","iopub.execute_input":"2025-07-22T10:09:56.470412Z","iopub.status.idle":"2025-07-22T10:09:56.485752Z","shell.execute_reply.started":"2025-07-22T10:09:56.470394Z","shell.execute_reply":"2025-07-22T10:09:56.485090Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"import torch\n\ndef generate(\n    model,\n    tokenizer,           # your tokenizer to decode IDs to text\n    device,\n    start_tokens,        # list of ints, e.g. [BOS] or your prompt encoded\n    max_new_tokens=50,   # how many tokens to generate\n    block_size=64,       # context window\n    temperature=1.0,     # controls randomness\n    top_k=None           # optional: restrict to top-k for more randomness control\n):\n    model.eval()\n    generated = start_tokens.copy()\n    input_ids = torch.tensor(start_tokens, dtype=torch.long).unsqueeze(0).to(device)  # (1, len)\n\n    for _ in range(max_new_tokens):\n        # If context is longer than block_size, crop oldest tokens\n        if input_ids.size(1) > block_size:\n            input_ids = input_ids[:, -block_size:]\n\n        with torch.no_grad():\n            logits = model(input_ids)  # (1, seq_len, vocab_size)\n\n        # Get logits for last token only\n        logits = logits[:, -1, :] / temperature  # (1, vocab_size)\n\n        # Optionally top-k filter\n        if top_k is not None:\n            v, ix = torch.topk(logits, top_k)\n            logits[logits < v[0, -1]] = -float('Inf')\n\n        probs = torch.softmax(logits, dim=-1)  # (1, vocab_size)\n\n        next_token = torch.multinomial(probs, num_samples=1)  # (1, 1)\n\n        next_token_id = next_token.item()\n\n        # Append to sequence\n        generated.append(next_token_id)\n\n        # Update input_ids\n        input_ids = torch.cat([input_ids, next_token], dim=1)  # (1, seq_len+1)\n\n\n    return generated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:13:01.486550Z","iopub.execute_input":"2025-07-22T10:13:01.487410Z","iopub.status.idle":"2025-07-22T10:13:01.494034Z","shell.execute_reply.started":"2025-07-22T10:13:01.487385Z","shell.execute_reply":"2025-07-22T10:13:01.493271Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"# Suppose your tokenizer encodes/decodes:\nstart_text = \"Once upon a midnight dreary\"\nstart_tokens = tokenizer.encode(start_text).ids  # Or whatever you used\n\noutput_tokens = generate(\n    model=model,\n    tokenizer=tokenizer,\n    device=device,\n    start_tokens=start_tokens,\n    max_new_tokens=100,\n    block_size=64,\n    temperature=1.5,\n    top_k=50  # try top_k for more diverse output\n)\n\n# Decode\noutput_text = tokenizer.decode(output_tokens)\nprint(output_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:15:10.437687Z","iopub.execute_input":"2025-07-22T10:15:10.438348Z","iopub.status.idle":"2025-07-22T10:15:10.582058Z","shell.execute_reply.started":"2025-07-22T10:15:10.438322Z","shell.execute_reply":"2025-07-22T10:15:10.581303Z"}},"outputs":[{"name":"stdout","text":" Once upon a midnight dreary.  He lived in an organized. Every day - he wanted something wonderful and learn a valuable lesson â€”itating in the trees, trees, with them.\n\nThe vimp replied, \"I have some to pick this peas.\"\n\nCarikie thought for a moment every moment and then went on an adventure and went over it. Finally, there looked brighter, brighter for the lightning, no more teff. Paul wasn't alone anymore. \n\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"tokenizer.save(\"my_tokenizer.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T10:17:10.997330Z","iopub.execute_input":"2025-07-22T10:17:10.997938Z","iopub.status.idle":"2025-07-22T10:17:11.006709Z","shell.execute_reply.started":"2025-07-22T10:17:10.997913Z","shell.execute_reply":"2025-07-22T10:17:11.005980Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}